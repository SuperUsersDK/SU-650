Dette project er om et kursus i Generativ AI og Large Language Modeller

Her er kursus beskrivelse:

Få en praktisk introduktion til store sprogmodeller (LLM’er) og generativ AI – uden at skulle træne modeller selv. På dette kursus lærer du at arbejde med prompts, anvende API’er som OpenAI og Hugging Face, og integrere AI-funktionalitet i små Python-baserede applikationer. Perfekt til udviklere, dataanalytikere og AI-interesserede, der vil omsætte teknologien til konkrete løsninger.
Generativ AI og Large Language Models (LLM’er) har åbnet for helt nye muligheder inden for automatisering, tekstgenerering og dataanalyse. På dette kursus får du en grundlæggende, men praktisk introduktion til, hvordan LLM’er fungerer, og hvordan du kan bruge dem i Python-baserede projekter – uden at have en baggrund i maskinlæring.
Vi starter med at gennemgå, hvad LLM’er er, hvordan de adskiller sig fra klassisk machine learning, og hvordan man styrer deres output gennem parametre som temperatur og tokens. Du lærer at skrive præcise og effektive prompts (prompt engineering), teste og optimere dem, samt bruge strategier som few-shot og chain-of-thought prompting.
Derefter arbejder vi med API-adgang til populære modeller som OpenAI, Azure og Hugging Face. Vi bygger også små Python-løsninger, der integrerer LLM’er – fx chatbots, skriveassistenter eller kodehjælpere – med værktøjer som Streamlit eller Flask.
Kurset afsluttes med et fokus på etiske overvejelser og ansvarlig brug af AI, herunder håndtering af bias, hallucinationer og begrænsninger i output.
Efter kurset kan du:
* Forstå hvordan LLM’er fungerer og anvendes
* Skrive effektive og kontrollerede prompts
* Bruge OpenAI, Hugging Face eller lignende API’er
* Integrere LLM’er i små Python-baserede løsninger
* Styre output, temperatur, tokens og struktur
* Forholde dig til etiske aspekter og ansvarlig brug af AI

* MODUL 1  Introduktion til LLM og Generativ AI
    * LLM betyder "Large Language Model" og bruges når man ud fra sproglig tekst vil generere ny tekst
    * Hvad er LLM, og hvordan adskiller det sig fra klassisk ML?
    * Træning vs. inferens
    * Modelstørrelser, tokens og outputstyring

* MODUL 2 Prompt Engineering
    * Prompt engineering handler om at skrive spørgsmål og instruktioner, som LLM'en forstår og svarer godt på
    * Struktur: system, user og assistant prompts
    * Promptstrategier: few-shot, chain-of-thought
    * Outputformatering, styring og test 
* MODUL 3 API-brug og integration
    * En API er en slags fjernbetjening til at få adgang til en LLM-model via internettet
    * Brug af OpenAI, Azure og Hugging Face API
    * Python SDK’er og sikker adgang
    * Fejlhåndtering og timeout-strategier 
* MODUL 4 LLM i egne Python-løsninger
    * Vi ser på, hvordan man kan bruge LLM'er i sine egne små Python-programmer eller webapps
    * Chatbot, generativ skriveassistent eller kodehjælper
    * Integration med Streamlit, Flask eller terminal
    * Tilpasning til brugerinput og kontekst 
* MODUL 5  Etiske overvejelser og AI-sikkerhed
    * AI kan lave fejl og have indbyggede skævheder – det skal man lære at håndtere ansvarligt
    * Bias, hallucinationer og ansvarlig brug
    * Begrænsninger i output og styring af adfærd
    * Brug af rollebaserede instruktioner
# SU-650
